# 论文摘要

虽然 LLM 在语言理解和交互决策任务中展现出了令人印象深刻的性能，但它们的推理能力、行动能力一直被作为两个独立的课题进行研究。该论文探索了使用大语言模型以**交替的方式生成推理、行动**的方法，实现两者之间协同作用：推理帮助模型更新行动，处理异常情况；而行动则帮助与模型从外部源中获取信息，更新行动。

> 总结：
>
> 过去，将LLM的推理、行动两者独立研究。
>
> 现在，探索LLM以交替的方式生成推理轨迹、特定任务行动，从而实现推理、行动协同作用（推理让model更新行动、行动让model从外部获取信息提供给推理）。这就是ReAct。

在多个任务上应用 ReAct进行实验，证明了其效果优于目前最先进的基准模型，并提高了人类的可解释性和可信度。具体而言，在问答和事实核查任务中，ReAct 通过与简单的维基百科 API 交互，克服了思维链推理中普遍存在的幻觉、错误传播问题，并生成了类人化的任务解决轨迹，比没有推理轨迹的基准模型更具可解释性。在两个交互式决策基准测试中，ReAct 在仅需一两个上下文示例进行提示的情况下，表现优于模仿学习和强化学习方法。

# 相关研究

**思维链（Chain-of-Thought, CoT）：**揭示了 LLMs 为解决问题构建自身思考程序的能力。此后出现了一系列后续工作，包括用于解决复杂任务的**由简入繁提示（least-to-most prompting）**、**零样本思维链（zero-shot-CoT）**、**自一致性推理（self-consistency）**、**选择-推理（Selection-Inference）** 将推理过程分为选择和推理两个步骤、**STaR** 通过在模型自身生成的正确原理解释上进行微调，实现了推理过程的自举、**忠实推理（Faithful reasoning）**将多步推理分解为三个步骤，每一步分别由专门的语言模型执行。类似的方法如 **Scratchpad** 在中间计算步骤上微调语言模型，在多步计算问题上也显示出改进。

**Act-Only：WebGPT** 使用语言模型与网络浏览器交互、导航网页，并从复杂问题中推到答案。在对话建模中，像 **BlenderBot** 和 **Sparrow**  这样的聊天机器人，以及像 **SimpleTOD** 这样的面向任务的对话系统，也训练语言模型来做出关于 API 调用的决策。它们没有显式地对思考和推理过程建模，而是依赖于昂贵的人类反馈进行强化学习。

# 从人类处理一个问题的视角出发

人类智能的一个独特特征，是能够将以任务为导向的**行动**与语言**推理**无缝结合。以在厨房做饭为例：

**推理更新行动**上，在任意两个具体动作之间，我们可能会用语言进行推理，以便跟踪进度（“既然菜都切好了，我该烧开这锅水了”）、处理异常情况、根据情况调整计划（“我没有盐了，那就用酱油和胡椒代替吧”）、意识到何时需要外部信息（“面团怎么做？我上网查一下”）

**行动更新推理**上，我们也可能会通过行动（打开食谱阅读配方、打开冰箱、检查食材）来支持推理并回答问题（“我现在能做什么菜？”）。

这种行动与推理之间的紧密协同，使人类能够快速学习新任务，并在先前未见的状况或面临信息不确定性时，做出稳健的决策。

# 什么是ReAct——协同推理和行动

ReAct 的思想非常简单：我们将智能体的动作空间扩展为`*A*^=*A*∪*L*`，其中 `*L*`是语言空间。语言空间中的行动 `*a*^``*t*````∈*L*`（我们称之为**推理**）不会影响外部环境，因此不会产生观察反馈。相反，“推理” `*a*^``*t*```旨在通过对当前上下文 `*c*``*t*```进行推理来组合有用信息，并更新上下文 `*c*``*t*+1````=(*c*``*t*````,*a*^``*t*````)`以支持未来的行动。

大白话就是 ReAct 是一种结合推理与行动的通用范式，让 LLM 以交替的方式生成与任务相关的推理和行动，使得模型能够进行动态推理，以创建、维持和调整高层行动计划（**推理->行动/reason to act**），同时还能与外部环境（如维基百科）交互，将额外信息纳入推理中（**行动->推理/act to reason**）。

# 知识密集型推理任务的实验

对多步问答和事实核查等知识密集型推理任务进行实验。

## 实验设定

**数据集：**(1) **HotPotQA**，一个多步问答基准测试，要求对两个或更多维基百科段落进行推理；(2) **FEVER**，这是一个事实核查基准测试，每个声明根据是否存在可验证的维基百科段落被标注为支持、反驳、信息不足。

模型仅接收问题/声明作为输入，无法直接访问支撑段落，必须依赖其内部知识、与外部环境交互检索知识来支持推理。

**动作空间**：(1) search[entity]：如果对应实体的维基百科页面存在，则返回该页面的前 5 句；否则从维基百科搜索引擎中推荐前 5 个相似实体。(2) lookup[string]：返回页面中包含该字符串的下一句，模拟浏览器上的 Ctrl+F 功能。(3) finish[answer]：给出答案并完成当前任务。

**基准模型**：(a) **标准提示（Standard）**：删除 ReAct 轨迹中所有的思考、行动和观察。(b) **思维链提示（CoT）**删除行动和观察，作为仅推理的基准。还构建了一个**自一致性基准（CoT-SC）**，执行21 条 CoT 推理，通过多数投票得到答案。(c) **仅行动提示（Act）**：删除 ReAct 轨迹中的思考。

**微调模型（Finetuning）**：使用 ReAct 生成的 3000 条带有正确答案的轨迹，对较小的语言模型（PaLM-8/62B）进行微调，使其能够根据输入的问题/声明解码出完整的轨迹（包含所有思考、动作和观察）。

## 过程抽象

![img](https://h5.static.yximgs.com/udata/pkg/EE-KSTACK/d333902bd99a25878db0fc1fb93d350a.png)

图1

**问题：**除了苹果遥控器之外，还有什么设备可以控制苹果遥控器最初设计用来交互的那个程序？

**标准：**iPod

**COT（仅推理）：**模型先经过思考（让我们一步步思考。Apple Remote 最初被设计用来与 Apple TV 进行交互。Apple TV 可以通过 iPhone、iPad 和 iPod Touch 来控制。因此，答案是 iPhone、iPad 和 iPod Touch。），然后给出答案iPhone、iPad、iPod Touch。这是一个典型的模型幻觉案例。实际上，Apple Remote 最初是为 Mac 上的软件设计的，而模型错误地认为它是为 Apple TV 设计的。

**Act-Only（仅行动）：**经过一系列的行动-观察

行动 1： 搜索 [Apple Remote]
观察 1： Apple Remote 是一款遥控器……
行动 2： 搜索 [Front Row]
观察 2： 未能找到 [Front Row]。相似条目：……
行动 3： 搜索 [Front Row (software)]
观察 3： Front Row 是一款已停产的媒体中心软件……
行动 4： 完成 [是]（Finish[yes]）

由于缺乏思考步骤，模型的行动显得比较机械，且最后的完成动作并没有给出具体的设备名称，这体现了纯行动模式在处理复杂逻辑时的局限性。

**ReAct（推理+行动）：**经过一系列的推理<->行动

思考 1： 我需要搜索 Apple Remote，并找到它最初设计用来交互（配合使用）的程序。
行动 1： 搜索 [Apple Remote]
观察 1： Apple Remote 是苹果公司在 2005 年 10 月推出的一款遥控器……最初设计用于控制 Front Row 媒体中心程序……

思考 2： Apple Remote 最初是为控制 Front Row 媒体中心程序而设计的。接下来我需要搜索 Front Row，并找到还有什么其他设备可以控制它。
行动 2： 搜索 [Front Row]
观察 2： 未能找到 [Front Row]。相似词条：['Front Row Seat to Earth', 'Front Row Motorsports', 'Front Row (software)', ...]

思考 3： 没找到 Front Row。我需要搜索 Front Row (software)。
行动 3： 搜索 [Front Row (software)]
观察 3： Front Row 是一款已停产的媒体中心软件……

思考 4： Front Row (软件) 由 Apple Remote 或键盘功能键控制。所以答案是键盘功能键。
行动 4： 完成 [键盘功能键] 

## 结果

![img](https://h23.static.yximgs.com/udata/pkg/EE-KSTACK/1fb951d92d4a8c4bb6fc0a168ef5ca17.png)

**ReAct 持续优于 Act**：ReAct 在这两个任务上都优于 Act，证明了推理在指导行动方面的价值，特别是在综合最终答案方面。

**ReAct vs. CoT**：ReAct 在 Fever 上的表现优于 CoT，而在 HotpotQA 上略逊于 CoT。

在Fever上ReAct优于CoT的原因：幻觉是 CoT 的严重问题，这导致其在成功模式下的误报率远高于 ReAct，并构成了其主要的失败模式。相比之下，得益于外部知识库的访问，ReAct 的解题轨迹更接地气、受事实驱动且更值得信赖。 

在HotpotQA上CoT优于ReAct的原因：虽然交替进行的推理、行动和观察步骤提高了 ReAct 的落地性和可靠性，但这种结构化约束也降低了其构建推理步骤的灵活性，导致推理错误率高于 CoT。ReAct 有一种特有的频繁错误模式，模型会重复生成之前的思考和行动，我们将其归类为“推理错误”，因为模型未能推断出下一步应采取的正确行动以跳出循环。此外，检索质量至关重要，对于 ReAct，通过搜索成功检索到有信息量的知识至关重要。“无信息搜索”会误导模型推理，使其难以恢复和重构思考。

**ReAct + CoT-SC 在提示大语言模型时表现最佳**：HotpotQA 和 Fever 上表现最好的提示方法分别是 ReAct → CoT-SC 和 CoT-SC → ReAct。

**ReAct 在微调中表现最佳**：下图是在 HotpotQA 上提示/微调四种方法（Standard, CoT, Act, ReAct）的规模效应。仅使用提示词情况下，由于很难仅从上下文示例中同时学习推理和行动，ReAct 的提示效果在四种方法中垫底。然而，当使用仅 3000 个示例进行微调时，ReAct 成为了四种方法中效果最好的。我们可以认为使用数据进行微调可能是释放 ReAct 能力的更好途径。

![img](https://h23.static.yximgs.com/udata/pkg/EE-KSTACK/4514c61b55acfc7f28e719b2486f41d5.png)

# 决策任务的实验

对 ALFWorld 和 WebShop 两个基于语言的交互式决策任务进行实验

## 实验设定

**数据集：**（1）**ALFWorld** 是一个合成的文本游戏。它包含 6 类任务，智能体需要通过文本动作在模拟的家庭环境中导航并进行交互，以实现高层目标（如：在台灯下检查文件）。基准模型使用 **BUTLER** ，这是一个在每类任务上使用 10⁵ 条专家轨迹训练的模仿学习智能体。（2）**WebShop，**是一个最近提出的在线购物网站环境，WebShop 包含高度多样化的结构化和非结构化文本（例如从亚马逊爬取的商品标题、描述和选项），并要求智能体根据用户指令（例如：“我正在寻找一个带抽屉的床头柜，镍色饰面，价格低于 140 美元”）通过网络交互（例如：搜索“nightstand drawers”，选择“color: modern-nickel-white”或“back to search”等按钮）购买产品。该任务通过 500 条测试指令的平均分（所选产品覆盖所需属性的百分比在所有回合中的平均值）和成功率（所选产品满足所有要求的次数占比）进行评估。将其与使用 1012 条人工标注轨迹训练的模仿学习（IL）方法，以及额外使用 10587 条训练指令训练的“模仿+强化学习”（IL + RL）方法进行了比较。

## 过程抽象

![img](https://h23.static.yximgs.com/udata/pkg/EE-KSTACK/9dac39857639ed03480820a5a34a6881.png)

图2

**场景描述：**你现在处于房间中央。环顾四周，你看到了橱柜 6、橱柜 1、咖啡机 1、台面 3、炉灶炉头 1 和烤面包机 1。你的任务是将一个胡椒粉罐放进抽屉里。

**Act-Only：**

行动 1： 前往抽屉
观察 1： 抽屉 1 是关闭的。
行动 2： 打开抽屉 1
观察 2： 你打开了抽屉 1。抽屉 1 现已开启。在里面，你看到了一个洗碗海绵 2 和一把勺子 1。
行动 3： 前往水槽
观察 3： 在水槽上，你看到了一个洗碗海绵 3、一把铲子 1 和一把勺子 2。
行动 4： 从水槽拿取胡椒粉罐 1
观察 4： 没有任何反应。
行动 5： 从水槽拿取胡椒粉罐
观察 5： 没有任何反应。

由于缺乏思考步骤， 在“观察 3”中，系统已经明确告知水槽上只有海绵、铲子和勺子，并没有胡椒粉罐，模型还在“行动 4”和“行动 5”中试图拿取一个并不存在的物体，导致任务停滞不前。

**ReAct：**

行动 1：思考 [首先我需要找到一个胡椒粉罐……它更有可能出现在橱柜 (1-6) 或台面 (1-3) 上。]
行动 2： 前往橱柜 1
观察 2： 在橱柜 1 上，你看到了一个花瓶 2。
（此处代理先后前往了橱柜 1、2、3，接着前往了台面 1 和 2）
行动 6： 前往台面 3
观察 6： 在台面 3 上，你看到了一个苹果 1、一块面包 1、一个胡椒粉罐 1 以及一个花瓶 3。
行动 7： 从台面 3 拿取胡椒粉罐 1
观察 7： 你从台面上拿起了胡椒粉罐 1。
行动 8：思考 [现在我找到了胡椒粉罐 1。下一步，我需要把它放入/放在抽屉 1 中。]
行动 9： 前往抽屉 1
观察 9： 抽屉 1 是关闭的。
行动 10： 打开抽屉 1
观察 10： 你打开了抽屉 1。
行动 11： 将胡椒粉罐 1 放入/放在抽屉 1
观察 11： 你将胡椒粉罐 1 放入/放在了抽屉 1 中。

## 结果

在 ALFWorld 和 Webshop上，ReAct 的表现均优于 Act。
在 ALFWorld 上从定性分析来看，我们发现没有任何“思考”的 Act 模型无法正确地将目标分解为更小的子目标，或者会丢失对当前环境状态的跟踪。

在 WebShop 上，单样本的 Act 提示表现就已经与 IL 和 IL+RL 方法相当。通过检查示例发现 ReAct 更有可能通过推理来弥合噪声观察与行动之间的差距，从而识别出与指令相关的产品和选项。然而，现有方法与专家人类的表现仍有差距，人类会进行显著更多的产品探索和查询重构，这对于基于提示词的方法来说仍然具有挑战性。 

# 总结

CoT：只使用内部信息思考会造成严重的幻觉

Act-Only：只行动

ReAct：协同推理与行动

微调可能是释放 ReAct 潜力的更好途径

可以尝试 ReAct 和 CoT-SC 结合，并让模型根据以下启发式策略决定何时切换方法：ReAct → CoT-SC， 当 ReAct 未能在给定步数内返回答案时，退避到 CoT-SC。 CoT-SC → ReAct，当 n 个 CoT-SC 样本中的多数答案出现次数少于 n/2 次时，退避到 ReAct

# 附录

[paper](原文链接：https://arxiv.org/pdf/2210.03629)

[代码](https://github.com/ysymyth/ReAct)

 